{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dad26a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Dict, List\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from projectaria_tools.core.sensor_data import ImageData\n",
    "from projectaria_tools.core.stream_id     import StreamId\n",
    "from projectaria_tools.core.sophus        import SE3, SO3\n",
    "from projectaria_tools.core               import data_provider\n",
    "from nymeria.data_provider      import NymeriaDataProvider\n",
    "from nymeria.recording_data_provider import RecordingDataProvider, AriaStream\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Normalize, Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96247f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NymeriaPoseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns for each index\n",
    "        • RGB frame  (C,H,W)  float32  [0‑1]\n",
    "        • 3‑D joints in camera frame (23,3)\n",
    "        • 2‑D projected joints        (23,2)   (‑1 if outside)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_root: Path,\n",
    "        transform: Optional[Callable] = None,\n",
    "        half: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_root = Path(seq_root)\n",
    "        self.transform = transform\n",
    "        self.half = half\n",
    "\n",
    "        # --- Nymeria providers -------------------------------------------------\n",
    "        self.dp = NymeriaDataProvider(\n",
    "            sequence_rootdir=self.seq_root, load_wrist=False, load_observer=False\n",
    "        )\n",
    "        self.rec_head: RecordingDataProvider = self.dp.recording_head\n",
    "        assert self.rec_head and self.rec_head.has_rgb, \"no RGB stream found\"\n",
    "\n",
    "        # --- VRS & calibration -------------------------------------------------\n",
    "        self.rgb_sid = StreamId(AriaStream.camera_rgb.value)\n",
    "        self.vrs_dp = self.rec_head.vrs_dp\n",
    "        self._num_frames = self.vrs_dp.get_num_data(self.rgb_sid)\n",
    "\n",
    "        self.cam_calib = (\n",
    "            self.vrs_dp.get_device_calibration().get_camera_calib(\"camera-rgb\")\n",
    "        )\n",
    "        w, h = self.cam_calib.get_image_size()\n",
    "        if half:\n",
    "            w, h = w // 2, h // 2\n",
    "        self.img_size = (h, w)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def __len__(self) -> int:\n",
    "        return self._num_frames\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    @staticmethod\n",
    "    def _unique_joints(bones: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"XSens bones → 23 distinct joint positions.\"\"\"\n",
    "        pts = np.zeros((23, 3), np.float32)\n",
    "        for b, (child, parent) in enumerate(bones):\n",
    "            pts[b + 1] = child\n",
    "            if b == 0:\n",
    "                pts[0] = parent\n",
    "        return pts\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def __getitem__(self, idx: int):\n",
    "        # ---------- RGB & meta -----------------------------------------\n",
    "        img_data, meta = self.vrs_dp.get_image_data_by_index(self.rgb_sid, idx)\n",
    "\n",
    "        # -------- time‑code timestamp ----------------------------------\n",
    "        if hasattr(meta, \"time_code_timestamp_ns\"):\n",
    "            t_code_ns = meta.time_code_timestamp_ns\n",
    "        elif hasattr(self.vrs_dp, \"convert_device_time_to_timecode_time_ns\"):\n",
    "            t_code_ns = self.vrs_dp.convert_device_time_to_timecode_time_ns(\n",
    "                meta.capture_timestamp_ns\n",
    "            )\n",
    "        else:  # fallback – device‑time\n",
    "            t_code_ns = meta.capture_timestamp_ns\n",
    "\n",
    "        # -------- ground‑truth poses -----------------------------------\n",
    "        poses = self.dp.get_synced_poses(t_code_ns)\n",
    "        bones = poses[\"xsens\"]  # (22,2,3)\n",
    "        joints_w = self._unique_joints(bones)  # (23,3)\n",
    "\n",
    "        # -------- world → camera transform -----------------------------\n",
    "        T_W_D: SE3 = poses[\"recording_head\"].transform_world_device\n",
    "        T_D_C: SE3 = self.cam_calib.get_transform_device_camera()\n",
    "\n",
    "        R_WD = T_W_D.rotation().to_matrix()\n",
    "        t_WD = T_W_D.translation().ravel()  # (3,)\n",
    "        R_DC = T_D_C.rotation().to_matrix()\n",
    "        t_DC = T_D_C.translation().ravel()  # (3,1)\n",
    "\n",
    "        R_WC = R_WD @ R_DC\n",
    "        t_WC = (R_WD @ t_DC).ravel() + t_WD\n",
    "\n",
    "        # joints in camera frame\n",
    "        joints_c = (R_WC.T @ (joints_w - t_WC).T).T  # (23,3)\n",
    "\n",
    "        # -------- 2‑D projection ---------------------------------------\n",
    "        uv = []\n",
    "        for p in joints_c:\n",
    "            pix = self.cam_calib.project(p)\n",
    "            if pix is None:\n",
    "                uv.append([-1.0, -1.0])\n",
    "            else:\n",
    "                u, v = pix\n",
    "                if self.half:\n",
    "                    u, v = u / 2.0, v / 2.0\n",
    "                uv.append([u, v])\n",
    "        joints_2d = torch.tensor(uv, dtype=torch.float32)  # (23,2)\n",
    "\n",
    "        # -------- RGB tensor -------------------------------------------\n",
    "        arr = img_data.to_numpy_array()\n",
    "        if self.half:\n",
    "            arr = arr[::2, ::2]\n",
    "        frame = torch.from_numpy(arr).permute(2, 0, 1).float() / 255.0\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "\n",
    "        return frame, torch.from_numpy(joints_c).float(), joints_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3856195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-07 15:19:24.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mloading xsens from npzfile='C:\\\\Users\\\\Damir\\\\nymeria_dataset\\\\d\\\\20230622_s0_john_solomon_act2_8urygm\\\\body\\\\xdata.npz'\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_qWXYZ', v.shape=(144289, 92)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_tXYZ', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_velocity', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_acceleration', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_angularVelocity', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_angularAcceleration', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='foot_contacts', v.shape=(144289, 4)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='sensor_qWXYZ', v.shape=(144289, 68)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='sensor_freeAcceleration', v.shape=(144289, 51)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='joint_angleEulerZXY', v.shape=(144289, 66)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='timestamps_us', v.shape=(144289,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='frame_index', v.shape=(144289,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='identity_segment_qWXYZ', v.shape=(92,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='identity_segment_tXYZ', v.shape=(69,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='tpose_segment_qWXYZ', v.shape=(92,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='tpose_segment_tXYZ', v.shape=(69,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='tpose-isb_segment_qWXYZ', v.shape=(92,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='tpose-isb_segment_tXYZ', v.shape=(69,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segmentCount', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='sensorCount', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='jointCount', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='frameRate', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='frameCount', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.828\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__correct_timestamps\u001b[0m:\u001b[36m65\u001b[0m - \u001b[33m\u001b[1mnumber of invalid timestamps 6, percentage=np.float64(0.004158321147142194)%\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:24.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__correct_timestamps\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mafter correct t_diff[-1]= np.int64(4296)us\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:26.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.data_provider\u001b[0m:\u001b[36m__get_timespan_ns\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mtime span: t_start= 3847697100000us t_end= 4446894896000us duration= 599.197796s\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:26.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.data_provider\u001b[0m:\u001b[36m__compute_xsens_to_aria_alignment\u001b[0m:\u001b[36m217\u001b[0m - \u001b[1mcompute alignment from xsens head to aria headset\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:26.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36mget_T_w_h\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mget 143328 samples for computing alignment\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:37.640\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m36\u001b[0m - \u001b[34m\u001b[1mA.shape=(3, 143326), B.shape=(3, 143326)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:37.648\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m41\u001b[0m - \u001b[34m\u001b[1mmatrixU.shape=(3, 3), S.shape=(3,), matrixVh.shape=(3, 3)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:38.808\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mjacobian.shape=(429978, 3)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:39.782\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mresidual.shape=(429978, 1)\u001b[0m\n",
      "\u001b[32m2025-05-07 15:19:39.795\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1mT_A_B=array([[ 0.03052879, -0.06998607, -0.99708071,  0.0688561 ],\n",
      "       [-0.64812185,  0.75802483, -0.07305082,  0.03089059],\n",
      "       [ 0.76092448,  0.64845995, -0.02221792, -0.12410255]])\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "root    = Path(r\"C:\\Users\\Damir\\nymeria_dataset\\d\"\n",
    "               r\"\\20230622_s0_john_solomon_act2_8urygm\")\n",
    "norm = Compose([Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])])\n",
    "ds      = NymeriaPoseDataset(root, transform=norm, half=True)\n",
    "loader  = DataLoader(ds, batch_size=8, shuffle=True,\n",
    "                     num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e11c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-07 15:19:40.090\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnymeria.data_provider\u001b[0m:\u001b[36mget_synced_poses\u001b[0m:\u001b[36m188\u001b[0m - \u001b[33m\u001b[1mtime difference for pose query -27712260.084046 ms\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ---- grab one sample (first frame) ---------------------------------\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m img_t, joints_3d, joints_2d \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m          \u001b[38;5;66;03m# (C,H,W), (23,3), (23,2)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# undo the Normalize() so colours look right\u001b[39;00m\n\u001b[0;32m      8\u001b[0m img_t_vis \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(img_t,\n\u001b[0;32m      9\u001b[0m                         mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],   \u001b[38;5;66;03m# invert the earlier (x-0.5)/0.5\u001b[39;00m\n\u001b[0;32m     10\u001b[0m                         std \u001b[38;5;241m=\u001b[39m[ \u001b[38;5;241m0.5\u001b[39m,  \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m])\n",
      "Cell \u001b[1;32mIn[44], line 86\u001b[0m, in \u001b[0;36mNymeriaPoseDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     83\u001b[0m t_DC \u001b[38;5;241m=\u001b[39m T_D_C\u001b[38;5;241m.\u001b[39mtranslation()  \u001b[38;5;66;03m# (3,1)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m R_WC \u001b[38;5;241m=\u001b[39m R_WD \u001b[38;5;241m@\u001b[39m R_DC\n\u001b[1;32m---> 86\u001b[0m t_WC \u001b[38;5;241m=\u001b[39m (\u001b[43mR_WD\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_DC\u001b[49m)\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;241m+\u001b[39m t_WD\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# joints in camera frame\u001b[39;00m\n\u001b[0;32m     89\u001b[0m joints_c \u001b[38;5;241m=\u001b[39m (R_WC\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (joints_w \u001b[38;5;241m-\u001b[39m t_WC)\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# (23,3)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 3)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# ---- grab one sample (first frame) ---------------------------------\n",
    "img_t, joints_3d, joints_2d = ds[0]          # (C,H,W), (23,3), (23,2)\n",
    "\n",
    "# undo the Normalize() so colours look right\n",
    "img_t_vis = F.normalize(img_t,\n",
    "                        mean=[-1, -1, -1],   # invert the earlier (x-0.5)/0.5\n",
    "                        std =[ 0.5,  0.5, 0.5])\n",
    "\n",
    "img_np = img_t_vis.permute(1,2,0).numpy()    # → HWC in [0,1]\n",
    "\n",
    "# ---- skeleton connectivity -----------------------------------------\n",
    "parents = [\n",
    "    -1,  # Pelvis has no parent\n",
    "     0,1,2,3,4,5,4,7,8,9,4,11,12,13,0,15,16,17,0,19,20,21\n",
    "]                                          # from XSensConstants.kintree_parents\n",
    "\n",
    "# ---- draw -----------------------------------------------------------\n",
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "ax.imshow(img_np)\n",
    "ax.axis('off')\n",
    "\n",
    "# joints_2d is (u,v) ; split for convenience\n",
    "u,v = joints_2d[:,0].numpy(), joints_2d[:,1].numpy()\n",
    "\n",
    "# draw bones\n",
    "for i,p in enumerate(parents):\n",
    "    if p < 0:           # root\n",
    "        continue\n",
    "    if u[i] < 0 or u[p] < 0:   # one of the points was outside image\n",
    "        continue\n",
    "    ax.plot([u[i],u[p]], [v[i],v[p]], c='lime', lw=2)\n",
    "\n",
    "# draw joints\n",
    "ax.scatter(u[v>=0], v[v>=0], c='red', s=15)\n",
    "\n",
    "plt.title(\"Frame 0 with projected XSens skeleton\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
