{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad26a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Dict, List\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from projectaria_tools.core.sensor_data import ImageData\n",
    "from projectaria_tools.core.stream_id     import StreamId\n",
    "from projectaria_tools.core.sophus        import SE3, SO3\n",
    "from projectaria_tools.core               import data_provider\n",
    "from nymeria.data_provider      import NymeriaDataProvider\n",
    "from nymeria.recording_data_provider import RecordingDataProvider, AriaStream\n",
    "from torchvision.transforms import Compose, Normalize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96247f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NymeriaPoseDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_root   : Path,                       \n",
    "        transform  : Optional[Callable] = None,  \n",
    "        half       : bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_root  = Path(seq_root)\n",
    "        self.transform = transform\n",
    "        self.half      = half\n",
    "        self.dp  = NymeriaDataProvider(sequence_rootdir=self.seq_root, load_wrist=False, load_observer=False)\n",
    "        self.rec_head : RecordingDataProvider = self.dp.recording_head\n",
    "        assert self.rec_head and self.rec_head.has_rgb, \"no RGB stream found\"\n",
    "        self.rgb_sid  = StreamId(AriaStream.camera_rgb.value)\n",
    "        self.vrs_dp   = self.rec_head.vrs_dp\n",
    "        self._num_frames = self.vrs_dp.get_num_data(self.rgb_sid)\n",
    "        self.cam_calib  = self.vrs_dp.get_device_calibration().get_camera_calib(\"camera-rgb\")\n",
    "        w, h = self.cam_calib.get_image_size()\n",
    "        if half: w, h = w//2, h//2\n",
    "        self.img_size = (h, w)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._num_frames\n",
    "\n",
    "    @staticmethod\n",
    "    def _unique_joints(bones: np.ndarray) -> np.ndarray:\n",
    "        pts = np.zeros((23, 3), np.float32)\n",
    "        for b,(c,p) in enumerate(bones):\n",
    "            pts[b+1] = c         \n",
    "            pts[0] = p if b==0 else pts[0]  \n",
    "        return pts\n",
    "\n",
    "    def __getitem__(self, idx : int):\n",
    "        img_data, meta, _ = self.vrs_dp.get_image_data_by_index(self.rgb_sid, idx)\n",
    "        t_code_ns = self.vrs_dp.convert_from_device_to_timecode_time_ns(meta.capture_timestamp_ns)\n",
    "        poses = self.dp.get_synced_poses(t_code_ns)\n",
    "        bones = poses[\"xsens\"]                   \n",
    "        joints_w = self._unique_joints(bones)\n",
    "        T_W_D : SE3 = poses[\"recording_head\"].transform_world_device\n",
    "        T_D_C : SE3 = self.cam_calib.get_transform_device_camera()\n",
    "        T_W_C  = T_W_D * T_D_C\n",
    "        R_WC   = SO3(T_W_C.rotation()).as_matrix()\n",
    "        t_WC   = T_W_C.translation()\n",
    "        joints_c = (R_WC.T @ (joints_w - t_WC).T).T\n",
    "        uv = []\n",
    "        for p in joints_c:\n",
    "            pix = self.cam_calib.project(p)\n",
    "            if pix is None:\n",
    "                uv.append([-1,-1])\n",
    "            else:\n",
    "                u,v = pix\n",
    "                if self.half:\n",
    "                    u, v = u/2.0, v/2.0\n",
    "                uv.append([u,v])\n",
    "        joints_2d = torch.tensor(uv, dtype=torch.float32)\n",
    "        arr = img_data.to_numpy_array()\n",
    "        if self.half:\n",
    "            arr = arr[::2, ::2]      \n",
    "        frame = torch.from_numpy(arr).permute(2,0,1).float() / 255.0\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "        return frame,                       \\\n",
    "               torch.from_numpy(joints_c).float(), \\\n",
    "               joints_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3856195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-07 14:41:20.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mloading xsens from npzfile='C:\\\\Users\\\\Damir\\\\nymeria_dataset\\\\d\\\\20230622_s0_john_solomon_act2_8urygm\\\\body\\\\xdata.npz'\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_qWXYZ', v.shape=(144289, 92)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_tXYZ', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_velocity', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_acceleration', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_angularVelocity', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segment_angularAcceleration', v.shape=(144289, 69)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='foot_contacts', v.shape=(144289, 4)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='sensor_qWXYZ', v.shape=(144289, 68)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='sensor_freeAcceleration', v.shape=(144289, 51)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='joint_angleEulerZXY', v.shape=(144289, 66)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='timestamps_us', v.shape=(144289,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='frame_index', v.shape=(144289,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='identity_segment_qWXYZ', v.shape=(92,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='identity_segment_tXYZ', v.shape=(69,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='tpose_segment_qWXYZ', v.shape=(92,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='tpose_segment_tXYZ', v.shape=(69,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='tpose-isb_segment_qWXYZ', v.shape=(92,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='tpose-isb_segment_tXYZ', v.shape=(69,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='segmentCount', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='sensorCount', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='jointCount', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='frameRate', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mk='frameCount', v.shape=(1,)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.741\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__correct_timestamps\u001b[0m:\u001b[36m65\u001b[0m - \u001b[33m\u001b[1mnumber of invalid timestamps 6, percentage=np.float64(0.004158321147142194)%\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:20.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36m__correct_timestamps\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mafter correct t_diff[-1]= np.int64(4296)us\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:22.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.data_provider\u001b[0m:\u001b[36m__get_timespan_ns\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mtime span: t_start= 3847697100000us t_end= 4446894896000us duration= 599.197796s\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:23.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.data_provider\u001b[0m:\u001b[36m__compute_xsens_to_aria_alignment\u001b[0m:\u001b[36m217\u001b[0m - \u001b[1mcompute alignment from xsens head to aria headset\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:23.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnymeria.body_motion_provider\u001b[0m:\u001b[36mget_T_w_h\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mget 143328 samples for computing alignment\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:31.202\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m36\u001b[0m - \u001b[34m\u001b[1mA.shape=(3, 143326), B.shape=(3, 143326)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:31.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m41\u001b[0m - \u001b[34m\u001b[1mmatrixU.shape=(3, 3), S.shape=(3,), matrixVh.shape=(3, 3)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:32.043\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mjacobian.shape=(429978, 3)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:32.926\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mresidual.shape=(429978, 1)\u001b[0m\n",
      "\u001b[32m2025-05-07 14:41:32.949\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnymeria.handeye\u001b[0m:\u001b[36mso3xR3\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1mT_A_B=array([[ 0.03052879, -0.06998607, -0.99708071,  0.0688561 ],\n",
      "       [-0.64812185,  0.75802483, -0.07305082,  0.03089059],\n",
      "       [ 0.76092448,  0.64845995, -0.02221792, -0.12410255]])\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "root    = Path(r\"C:\\Users\\Damir\\nymeria_dataset\\d\"\n",
    "               r\"\\20230622_s0_john_solomon_act2_8urygm\")\n",
    "norm = Compose([Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])])\n",
    "ds      = NymeriaPoseDataset(root, transform=norm, half=True)\n",
    "loader  = DataLoader(ds, batch_size=8, shuffle=True,\n",
    "                     num_workers=4, pin_memory=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
